---
layout: post
title: VAE in Natural Language Generation
categories: Weekly
tags: [NLG]
mathjax: true
---

# What is VAE?

VAE is short for Variational Auto-Encoder which is proposed by Kingma et al. in [Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf). It extends the Variational Bayesian (VB) method by deriving a tractable lower bound and a reparameterization trick that allows the bound to be optimized using stochastic gradient updating. 

<!-- more -->

## Bayesian Inference

Brought from [Wikipedia](https://en.wikipedia.org/wiki/Bayesian_inference):

> Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available.

The gist behind Bayesian inference is to approximate the **posterior probability** $P(H\mid E)$ of some hypothesis $H$ given the observed data (evidence) $E$. This posterior probability is often affected by the data and we often define the **prior probability** of the hypothesis before seeing any data as $P(H)$. The estimation of $P(H\mid E)$ is given by the Bayes' theorem:

\[
P(H\mid E)=\frac{P(E\mid H)P(H)}{P(E)}
\]

$P(E\mid H)$ is called the **likelihood**. It indicates the compatibility of the evidence with respect to the given hypothesis.

$P(E)$ is termed as the **marginal likelihood** or marginal evidence. This term describes the distribution of the data.

Different from 'frequentist' statsitics, Bayesian theory calls for the use of the posterior predictive distribution to do predictive inference, i.e., to predict the distribution of a new, unobserved data point.

## Variational Bayesian

Bayesian inference defines a set of probabilities. The term 'variation' actually comes from relaxing the integral e.g. of the marginal likelihood $p(x)=\int p(x\mid z)p(z)dz$.

> The key to the variational method is to approximate the integral with a simpler form that is tractable, forming a lower or upper bound. 


~~In the problem formulation, an assumption to the data is that they are generated by some continuous latent variables.~~

## *Variational Auto-Encoder* from a probability model perspective

In this scenario, some assumptions are posed to the datapoints $\mathbf{X}=\{\mathbf{x}_i\}_{i=1}^N$. 

- Each datapoint $\mathbf{x}_i$ is considered i.i.d, i.e. independent identically distributed.

- Each datapoint $\mathbf{x}_i$ is generated by a corresponding local latent random variable $\mathbf{z}_i$.

~~What is shared across the datapoints is the global parameter $\theta$ that parametrize the prior probability $p_\theta (\mathbf{z})$ from which the latent variable $\mathbf{z}$ is sampled.~~

In variational inference, out goal is to approximate the intractable posterior distribution of the latent variable given the observed data, i.e. $p(\mathbf{z}\mid \mathbf{x})$ with a simpler distribution chosen from a family of distributions $q_\lambda(\mathbf{z}\mid \mathbf{x})$ parameterized by $\lambda$. 

>$\lambda$ is the variational parameter that indexes the family distributions. For example, if $q$ were Gaussian, it would be the mean and variance of the latent variables for each datapoint $\lambda_{x_i} = (\mu_{x_i}, \sigma^2_{x_i})$.

### Amortizing Posterior Inference

In amortized inference, instead of introducing different $\lambda_i$ for each datapoint $\mathbf{x}_i$ which would result in a linearly growing number of parameters with respect to the datapoints, we use a neural network parametrized by $\phi$ to learn the variational parameter $\lambda$. 

So the number of parameters to be learned is reduced to constant with respect to the number of datapoints. And since the real parameter govern $q$ becomes $\phi$, we could rewrite the distribution into $q_\phi(\mathbf{z}|\mathbf{x})$.

### Mean-field Variational Inference (MFVI)

The Mean-field approximation assumes that $q_\phi(\mathbf{Z}\mid \mathbf{X})$ could be fully factorized which means that all latent variables are independent. 
 
\[
q_\phi(\mathbf{Z}\mid \mathbf{X})=\prod_{\mathbf{z}\in\mathbf{Z},\ 
\mathbf{x}\in\mathbf{X}} q_\phi (\mathbf{z}\mid \mathbf{x})
\]


### Evidence Lower Bound (ELBO)

To approximate the posterior distribution $p(\mathbf{z}\mid \mathbf{x})$, we take the tractable approach by minimizing the KL divergence between $q_\phi(\mathbf{z}\mid \mathbf{x})$ and $p(\mathbf{z}\mid \mathbf{x})$.

\[
\mathbb{KL}(q_\phi(\mathbf{z}|\mathbf{x})\|p(\mathbf{z}|\mathbf{x}))=\mathbf{E}_{z\sim q_\phi(\mathbf{z}\mid \mathbf{x})}[\log \frac{q_\phi(\mathbf{z}\mid \mathbf{x})}{p(\mathbf{z}\mid \mathbf{x})}]\\
=\mathbf{E}_q[q_\phi(\mathbf{z}\mid \mathbf{x})]-\mathbf{E}_q[p( \mathbf{x}, \mathbf{z})] + \log p(\mathbf{x})
\]

Rewrite the evidence as

\[
\log p(\mathbf{x}) =ELBO(\phi) + \mathbb{KL}(q_\phi(\mathbf{z}|\mathbf{x})\|p(\mathbf{z}|\mathbf{x}))
\]

Since the log evidence is a constant, maximizing the ELBO is equivilant to minimizing the KL term.

Let's rewrite the ELBO into a tractable form:

\[
ELBO(\phi)=\mathbf{E}_q[p( \mathbf{x}, \mathbf{z})] - \mathbf{E}_q[q_\phi(\mathbf{z}\mid \mathbf{x})]\\
=\mathbf{E}_{z\sim q_\phi}[p(\mathbf{x}\mid \mathbf{z})] - \mathbb{KL}(q_\phi(\mathbf{z}\mid \mathbf{x})\|p(\mathbf{z}))
\]

The likelihood $p(\mathbf{x}\mid \mathbf{z})$ parametrized by another neural network with parameter $\theta$ is called the *generative model*. Recall that the estimated posterior distribution $q_\phi(\mathbf{z}\mid \mathbf{x})$ parametrized by a neural network with parameter $\phi$ is called the *inference model*.
